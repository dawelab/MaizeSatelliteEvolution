##CentC networks
##inital script to make networks by sliding region
##two models are made to predict structure
##test data used are 10kb non-overlapping sliding windows from chr 2, 7, and 10 from NC350

#packages
library('dplyr')
library('reshape2')
library('stringr')
library(igraph)
library(caret)
library(ggplot2)

`%!in%` = Negate(`%in%`)
setwd("~/Desktop/Dawe")

## reading all the data in -- chr 2, 7, & 10 in NC350
## data generated by network_sliding_smooth.sh with 20kb bins, 5kb slide
path = "~/Desktop/Dawe"
data_files<- dir(path, pattern=".blat_sliding_20kb_5kb.sub", recursive=TRUE, full.names=TRUE) ##all the comparison files

All <- lapply(data_files,function(i){
  x<- read.table(i)
  y<- select( x, c( "V1", "V10", "V11", "V14","V15"))
}) 

len<- 20000
All2<- list()
All_networks<- list()
network_summary<- as.data.frame(matrix(nrow=length(All2), ncol = 13))
for( i in 1:length(All) ){
  colnames(All[[i]])<- c("match", "nam1", "len1", "nam2", "len2")
  All[[i]]$jacc<- round(All[[i]]$match/( All[[i]]$len1 + All[[i]]$len2 - All[[i]]$match ), 6)
  nam_val<- str_split(data_files[i], pattern="/", simplify=T)[,6]
  nam_start<- str_split(nam_val, pattern="_", simplify=T)[,1]
  nam_val2<-  str_split(nam_val, pattern=":", simplify=T)[,2]
  nam_array<-  str_split(nam_val2, pattern="[.]", simplify=T)[,1]
  nam_val3<-  str_split(nam_val, pattern=":", simplify=T)[,1]
  nam_chr<-  str_split(nam_val3, pattern="_", simplify=T)[,2]
  
  dat2<- aggregate(jacc ~ nam1 + nam2, All[[i]], max)
  rev_dat2<-dat2
  colnames(rev_dat2)<- c("nam2",  "nam1", "jacc")
  dat2_d<- rbind( dat2, rev_dat2)
  dat2_mat<- acast(dat2_d, nam1~nam2, value.var="jacc", fun.aggregate = max, fill=0.0000)
  for(j in 1:nrow( dat2_mat)){
    dat2_mat[j,j]<-1
  }
  All2[[i]]<- dat2_mat
  
  if( nrow(dat2_mat) > 1){
  dup<- findCorrelation(dat2_mat,cutoff = .9999999, names=TRUE) ##concat identical 
  keep<- colnames(dat2_mat)[colnames(dat2_mat) %!in% dup]
  
  nams<- as.data.frame(keep)
  }else{
    nams<- as.data.frame(dat2_mat)
    keep<- colnames(nams)
  }
  nams$count<- NA
  
  for( j in 1:nrow(nams)){
    row<-as.data.frame(dat2_mat[nams[j,1],])
    colnames(row)<- c("val")
    row$nam<- row.names(row)
    sub<-row[row$val==1,]
    nams$count[j]<-nrow(sub)
    max_ident<- max( nams$count)
  }
  
  if( length(keep) >1) {
    dat2_mat_sub<- dat2_mat[keep, keep]
    dat2_mat_sub[ dat2_mat_sub < .95] <- 0
    dat2_mat_sub[dat2_mat_sub >= .95] <- 1
    
    network2 <- graph_from_adjacency_matrix(dat2_mat_sub,  mode="undirected", diag=F, weighted = T)
    V(network2)$size <-8
    V(network2)$label <- NA
    
    V(network2)$label <- nams$count
    All_networks[[i]]<- network2
    group_counts<-as.data.frame(components(network2)$membership) 
    colnames( group_counts)<- c("V2")
    group_counts$V1<- row.names(group_counts)
    colnames( nams)<- c("V1", "V3")
    group_counts_or_names <- merge( group_counts, nams, by="V1") #V1 is the monomer, V2 is group, V3 is number of monomers
    count2<- group_counts_or_names %>% group_by(V2) %>% summarise(V3=sum(V3))
    count2_or<- count2[order(-count2$V3),]
    
    network_summary[i,1]<- nam_array
    network_summary[i,2]<-  nam_chr
    network_summary[i,3]<-  as.numeric(nam_start)-len
    network_summary[i,4]<- as.numeric(nam_start)
    network_summary[i,5]<- length(unique(All[[i]]$nam1)) #number of monomers
    network_summary[i,6]<- (length(dup)+ nrow( nams[nams$V3 > 1,]))/network_summary[i,5] #number of duplicate monomers
    network_summary[i,7]<- length(unique(components(network2)$membership))/network_summary[i,5] #number of separated clusters
    network_summary[i,8]<- count2_or$V3[1]/network_summary[i,5]
    network_summary[i,9]<- count2_or$V3[2]/network_summary[i,5]
    network_summary[i,10]<- modularity(network2, components(network2)$membership) #modularity 
    network_summary[i,11]<- mean_distance(network2, directed=F) #mean distance between nodes
    network_summary[i,12]<- mean(dat2_d$jacc)
    network_summary[i,13]<- max_ident/network_summary[i,5]
  } else{
    network_summary[i,1]<- nam_array
    network_summary[i,2]<-   nam_chr
    network_summary[i,3]<- as.numeric(nam_start)-len
    network_summary[i,4]<- as.numeric(nam_start)
    network_summary[i,5]<- length(unique(All[[i]]$nam1)) #number of monomers
    network_summary[i,6]<-(length(dup)+ nrow( nams[nams$V3 > 1,]))/network_summary[i,5] #number of duplicate monomers
    network_summary[i,7]<- 1 #number of separated clusters
    network_summary[i,8]<- 1
    network_summary[i,9]<- 0
    network_summary[i,10]<- 0#modularity 
    network_summary[i,11]<- 0 #mean distance between nodes
    network_summary[i,12]<- 1
    network_summary[i,13]<- 1
  }
}
colnames(network_summary)<- c("array","chr", "start", "end", "num_monomer", "dup_monomers", "uncon_clust", "prop_clust1", "prop_clust2", "modularity", "mean_dist", "avg_jacc", "max_contract")
network_summary_or<- network_summary[order(network_summary$chr, network_summary$array, network_summary$start), ]
write.csv( network_summary_or, "network_summary_or_chr2_10_7.csv", quote=F)


#hand check the structure for each data in this training data to determine classification
#data_files[39]
#plot( All_networks[[37]])
      
######################################
##classifications added to file by hand
class<- read.csv("network_summary_or_chr2_10_7_CLASS3.csv", header=T)
#prep variables and split data into training & test sets
class[is.na(class$mean_dist),]$mean_dist<- 10
class[is.na(class$prop_clust2),]$prop_clust2<- 0
class$Class<- as.factor(class$Class)

dt = sort(sample(nrow(class), nrow(class)*.8))
train<-class[dt,]
test<-class[-dt,]

##########################################
##MODEL BUILDING
# Classification Tree with rpart
library(rpart)

# grow tree 
fit <- rpart(Class ~dup_monomers+uncon_clust+prop_clust1+prop_clust2+modularity+mean_dist+avg_jacc+max_contract,method="class", data=train)
printcp(fit) # display the results 
plot(fit) # visualize cross-validation results 
plotcp(fit) # visualize cross-validation results 
summary(fit)# detailed summary of splits

# plot tree 
plot(fit, uniform=TRUE, 
     main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

test[ ,16] <- as.data.frame(predict(fit , newdata=test, type = 'class') )
test[ ,17] <- (test[ ,16]== test[ ,15]) 

##
library(MASS)
fit2 <- lda(Class~dup_monomers+uncon_clust+prop_clust1+prop_clust2+modularity+mean_dist+avg_jacc+max_contract, data=train, 
           na.action="na.omit")
test[ ,18] <-predict(fit2, newdata=test)$class
test[ ,19] <- (test[ ,18] == test[ ,15]) 
colnames( test)<- c( colnames(train), "Tree_Pred", "Tree_Match", "LDA_Pred", "LDA_Match")

test_sub<- test[test$Tree_Match == FALSE | test$LDA_Match == FALSE , ]
#8 misclassifications-- 6 by Tree model, 5 by LDA model-- out of 46 test
#.87 accuracy for tree, .85 accuracy for lda
# 3 windows where the moels do not agree with each other (.087)
# BUT-- lda does better with Expansions in general 
plot_dat<- predict(fit2, newdata=class)
plot_dat_class<- predict(fit2, newdata=class)$class
plot_dat_pos_nam<-  colnames(plot_dat$posterior)[max.col(plot_dat$posterior, ties.method = "first")] 
plot_dat_pos<-apply(plot_dat$posterior, 1, max)

plot_dat2<- as.data.frame(cbind( as.matrix(class$Class),as.matrix( plot_dat$x[,1:2])))
ggplot() + geom_point(data = plot_dat2, aes( x= as.numeric(LD1), y=as.numeric(LD2), color=V1))+
  theme_classic() +ylab("LD2")+xlab("LD1")+labs(color="Classification")+ggtitle("Chr 2, 7, & 10: Graph position by model, color by manual classification")

saveRDS(fit2, "lda_model.rds")
saveRDS(fit, "tree_model.rds")
